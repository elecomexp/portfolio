{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/cover_lander.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Autónomo (SmartCab)\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "El trabajo de un SmartCab es recoger al pasajero en un lugar y dejarlo en otro. Algunos detalles que nos encantaría que nuestro Smartcab tenga en cuenta serían:\n",
    "\n",
    "* Dejar al pasajero en la ubicación correcta.  \n",
    "* Ahorrar tiempo al pasajero dedicando el mínimo tiempo posible para dejarlo.  \n",
    "* Cuidar la seguridad del pasajero y respetar las normas de tráfico.\n",
    "\n",
    "En este Notebook se va a evaluar el aprendizaje del taxi autónomo sin refuerzo (movimientos aleatorios), frente al aprendizaje por refurezo mediante Q-Learning.\n",
    "\n",
    "## Recompensas\n",
    "\n",
    "* El agente debería recibir una alta recompensa positiva por una entrega del cliente exitosa porque este comportamiento es de los más importantes que queremos que aprenda.\n",
    "* El agente debería ser penalizado si intenta dejar a un pasajero en destinos incorrectos.\n",
    "* El agente debería recibir una ligera recompensa negativa por no llegar a destino después de cada intervalo de tiempo. \"Ligera\" negativa porque preferiríamos que nuestro agente llegue tarde en lugar de hacer movimientos erróneos tratando de llegar al destino lo más rápido posible.\"\n",
    "\n",
    "Para este ejercicio vamos a establecer las siguientes \"recompensas\":\n",
    "* Recibimos +20 puntos por un traslado exitoso.  \n",
    "* Perdemos 1 punto por cada intervalo de tiempo que tarda.  \n",
    "* También hay una penalización de 10 puntos por acciones de recogida y dejada ilegales. \n",
    "* Un movimiento que de lugar a un \"choque\" tiene la penalización de perder tiempo (el taxi no se desplaza pero se recibe la penalización de duración)\n",
    "\n",
    "## Espacio de estados\n",
    "\n",
    "El espacio de estados es el conjunto de todas las posibles situaciones en las que nuestro taxi podría estar. El estado además debe contener información útil que el agente necesite para tomar la acción correcta.\n",
    "\n",
    "<img src='../img/smartcab_map.png' width='350' />\n",
    "\n",
    "Supongamos que Smartcab es el único vehículo en este circuito de aprendizaje. Nuestro circuito está dividido en __una cuadrícula de 5x5__, lo que nos da __25 posibles ubicaciones__ para el taxi (posiciones (0,0) a (5,5)). Estas 25 ubicaciones __son una parte de nuestro espacio de estados__. Se puede observar que la __ubicación actual__ de nuestro taxi es la __coordenada (3, 1)__.\n",
    "\n",
    "También se puede ver que hay __cuatro (4) ubicaciones__ en las que podemos __recoger y dejar__ a un pasajero: R, G, Y, B o [(0,0), (0,4), (4,0), (4,3)] en coordenadas (fila, columna).  También debemos tener en cuenta un (1) estado adicional del pasajero de estar dentro del taxi.\n",
    "\n",
    "Por tanto, hay un total de 5 x 5 x 5 x 4 = 500 estados.\n",
    "\n",
    "## Espacio de acciones\n",
    "\n",
    "El agente se encuentra con uno de los 500 estados y toma una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger/dejar a un pasajero. En otras palabras, tenemos seis posibles acciones:\n",
    "\n",
    "0. sur\n",
    "1. norte\n",
    "2. este\n",
    "3. oeste\n",
    "4. recoger\n",
    "5. dejar\n",
    "\n",
    "Según la ilustración, el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes (por ejemplo mover de la posición (3,1) a la (3,0)).  El entorno proporciona una penalización de -1 por cada movimiento no permitido y el taxi no se moverá a ningún lado. Como siguiente estado devuelve el mismo de partida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Saltarnos la limitación de 200\n",
    "env = gym.make('Taxi-v3', render_mode='ansi').env\n",
    "\n",
    "state, info = env.reset(seed=19)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El cuadrado relleno representa el taxi, que es de color amarillo sin un pasajero y verde con un pasajero. En este caso, comenzaríamos con nuestro taxi situado en (2,0).\n",
    "* La barra (\"|\") representa una pared que el taxi no puede cruzar.\n",
    "* R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida del pasajero, y la letra morada es el destino actual. Es decir hay que recogerlo de Y y entregarlo en R, para este ejemplo generado al resetear el entorno con la semilla ajustada al valor 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado inicial: 208\n",
      "Info extra: {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)}\n"
     ]
    }
   ],
   "source": [
    "print('Estado inicial:', state)\n",
    "print('Info extra:', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 2, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desenvuelve el entorno base y llama al método decode en el entorno base\n",
    "list(env.unwrapped.decode(208))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieramos cambiar la posición de partida de nuestro taxi y llevarlo a la de la figura, podemos moverlo con las acciones e ignorar las recompensas. Bastaría con moverlo al este y luego al sur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mover el taxi al este (2) y luego al sur (0)\n",
    "movements = [2, 0]\n",
    "\n",
    "for action in movements:\n",
    "    env.step(action)\n",
    "    print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 1: Aprendizaje sin refuerzo. Movimientos aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasos de tiempo: 1444\n",
      "Penalizaciones: 460\n",
      "Tiempo real simulado: 79380\n"
     ]
    }
   ],
   "source": [
    "# Entorno con movimientos aleatorios\n",
    "timesteps = 0\n",
    "penalties, rewards = 0, 0\n",
    "simulated_time = 0\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    if action < 4:\n",
    "        simulated_time += 45\n",
    "    else:\n",
    "        simulated_time += 75\n",
    "    state, reward, done, truncated, info = env.step(action=action)\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    frames.append({\n",
    "        'frame': env.render(),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward,\n",
    "        'elapsed': simulated_time\n",
    "    })\n",
    "    timesteps += 1\n",
    "    \n",
    "print('Pasos de tiempo:', timesteps)\n",
    "print('Penalizaciones:', penalties)\n",
    "print('Tiempo real simulado:', simulated_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En tiempo de ejecución de código no ha sido nada. Pero si imaginamos que hubiera sido una prueba en un entorno real el resultado no sería aceptable. Esta es una de las razones por las que se usan entornos simulados para aprender. Aunque no siempre será posible o recomendable (por ejemplo, los coches autónomos de verdad aprenden sobre el terreno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización\n",
    "\n",
    "Para poder ver qué es lo que realmente está haciendo el taxi autónomo, hemos creado una función que pinta cada frame, dejando un intervalo de tiempo entre frame y frame para crear el efecto de animación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add \"src\" path\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..')) \n",
    "if os.path.exists(root_path) and root_path not in sys.path: \n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from utils.functions import episode_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 500\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Elapsed time (sec.): 79380\n"
     ]
    }
   ],
   "source": [
    "# Lanzamos la animacion, pero asegurándonos de que sólo mostramos como mucho 500 frames.\n",
    "# Si se desea ver completa sólo hay que quitar el indexado y volver a ejecutar\n",
    "episode_animation(frames[max(0,len(frames)-500):]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo se puede comprobar este mecanismo, aunque cumple con el cometido, tiene severas limitaciones:\n",
    "- Los tiempos son inaceptables.\n",
    "- No se guarda el \"aprendizaje\", pero aunque lo hiciéramos, sólo valdría para la situación de partida (el taxi en la posición 3,1. Recoger en \"Y\", y entregar en \"R\")\n",
    "\n",
    "Necesitamos un mecanismo de mejora de la estrategia que no sólo reduzca los tiempos y los optimice al máximo, sino que además sea flexible como para poder aplicarse en cualquier situación. Ese es el objetivo del Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 2: Aprendizaje por refuerzo\n",
    "\n",
    "Reiniciamos el entorno para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "Current State: 208\n",
      "Action Space: Discrete(6)\n",
      "State Space: Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# Reiniciar el entorno\n",
    "env = gym.make(\"Taxi-v3\", render_mode = \"ansi\").env\n",
    "state, info = env.reset(seed=19)\n",
    "\n",
    "# Renderizar el entorno\n",
    "print(env.render())\n",
    "\n",
    "# Imprimir el estado actual (devuelto por reset)\n",
    "print(\"Current State:\", state)\n",
    "\n",
    "# Imprimir el espacio de acciones\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "\n",
    "# Imprimir el espacio de observaciones\n",
    "print(\"State Space: {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "Estado: 228\n",
      "Recompensa: -1\n",
      "Terminó: False\n",
      "Truncado: False\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "Estado: 328\n",
      "Recompensa: -1\n",
      "Terminó: False\n",
      "Truncado: False\n"
     ]
    }
   ],
   "source": [
    "# Movimiento al Este y al Sur\n",
    "movements = [2, 0]\n",
    "\n",
    "for mov in movements:\n",
    "    # Realizar la acción\n",
    "    state, reward, done, truncated, info = env.step(mov)\n",
    "    \n",
    "    print(env.render())             # Mostrar el entorno\n",
    "    print(\"Estado:\", state)         # Mostrar el estado actual\n",
    "    print(\"Recompensa:\", reward)    # Mostrar la recompensa obtenida\n",
    "    print(\"Terminó:\", done)         # Mostrar si el episodio terminó\n",
    "    print(\"Truncado:\", truncated)   # Mostrar si el episodio fue truncado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Los pasos del algoritmo de Q-Learning epsilo-greedy que nos permitirá estimar la Q-table son los siguientes:\n",
    "\n",
    "* Inicializar la tabla Q con todos ceros.\n",
    "* Seleccionar los valores de los hiperparámetros\n",
    "* Comenzar a explorar acciones: Para cada estado, seleccione cualquiera entre todas las acciones posibles para el estado actual (S).\n",
    "* Viajar al siguiente estado (S') como resultado de esa acción (a).\n",
    "* Para todas las acciones posibles desde el estado (S') seleccione la que tenga el valor Q más alto.\n",
    "* Actualizar los valores de la tabla Q usando la ecuación ya vista.\n",
    "* Establecer el siguiente estado como el estado actual.\n",
    "* Si se alcanza el estado objetivo, entonces terminar y repetir el proceso.\n",
    "\n",
    "Lo primero creamos la estructura de datos que nos permita almacenar la Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from random import sample\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]], shape=(500, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar tabla-Q \n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(q_table.shape)\n",
    "print(q_table.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente es seleccionar los valores de los hiperparámetros, alpha, gamma y épsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear el algoritmo de entrenamiento que actualizará esta tabla Q a medida que el agente explore el entorno a lo largo de miles de episodios.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100000, 100.00\n",
      "Entrenamiento finalizado\n",
      "Tiempo de entrenamiento: 20.1 segundos\n"
     ]
    }
   ],
   "source": [
    "# Para ver el tiempo de CPU\n",
    "# %%time\n",
    "\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "num_episodes = 100_000\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "for i in range(1, num_episodes + 1):\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        next_max = np.max(q_table[next_state])   # maxQ(s', a')\n",
    "        old_value = q_table[state, int(action)]\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "        q_table[state, int(action)] = new_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "            \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Episodio: {i}, {i/num_episodes * 100:.2f}')\n",
    "    \n",
    "    state, info = env.reset()\n",
    "\n",
    "print('Entrenamiento finalizado')\n",
    "print(f'Tiempo de entrenamiento: {round(time.time() - init_time, 1)} segundos')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación\n",
    "\n",
    "Vamos a evaluar el rendimiento de nuestro agente. Ya no necesitamos explorar acciones, así que ahora la siguiente acción siempre se selecciona usando el mejor valor Q.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100, 100.00 %\n",
      "Resultados después de 100 episodios:\n",
      "Número medio de acciones por episodio: 13.22\n",
      "Número medio de penalizaciones por episodio: 0.0\n",
      "Recompensa media por episodio: 7.78\n"
     ]
    }
   ],
   "source": [
    "# Uso de la tabla-Q\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "num_episodes = 100\n",
    "\n",
    "state, info = env.reset(seed=19)\n",
    "\n",
    "# Tendrá un elemento por episodio que contendrá los \n",
    "# frames de ese episodio e información adicional\n",
    "set_frames = []\n",
    "\n",
    "for i in range(1, num_episodes + 1): \n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    frames = []\n",
    "    \n",
    "    while not done: \n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        frames.append({\n",
    "            'frame': env.render(),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'elapsed': 0\n",
    "        })\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "            total_penalties += 1\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait = True)\n",
    "        print(f\"Episodio: {i}, {i/num_episodes * 100:.2f} %\")\n",
    "        \n",
    "    set_frames.append(frames)\n",
    "    total_epochs += epochs\n",
    "    total_penalties += penalties\n",
    "    state, info = env.reset()\n",
    "    \n",
    "print(f'Resultados después de {num_episodes} episodios:')\n",
    "print(f'Número medio de acciones por episodio: {total_epochs / num_episodes}')\n",
    "print(f'Número medio de penalizaciones por episodio: {total_penalties / num_episodes}')\n",
    "print(f'Recompensa media por episodio: {total_rewards / num_episodes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores parecen bastante buenos, 13 (aproximadamente) pasos de duración media y no hay penalizaciones (no se intenta recoger o dejar al pasajero en sitios equivocados).\n",
    "\n",
    "Utilicemos ahora la visualización para ver cuanto de bien ha aprendido a conducir. Vamos a analizar 5 episodios escogidos aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 17\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Elapsed time (sec.): 0\n"
     ]
    }
   ],
   "source": [
    "for frame in sample(set_frames, 5):\n",
    "    episode_animation(frame[0:1])\n",
    "    sleep(2)\n",
    "    episode_animation(frame[1:])\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando nuestro agente de Q-learning con no usar aprendizaje por refuerzo\n",
    "\n",
    "Vamos a evaluar a nuestros agentes de acuerdo con las siguientes métricas,\n",
    "\n",
    "* Número promedio de penalizaciones por episodio: Cuanto menor sea el número, mejor será el rendimiento de nuestro agente. Idealmente, nos gustaría que esta métrica sea cero o muy cercana a cero.\n",
    "* Número promedio de pasos por episodio: También queremos que sea un valor pequeño, que nuestro agente tome la ruta más corta para llegar al destino.\n",
    "* Recompensas promedio por movimiento: Una recompensa más grande significa que el agente está haciendo lo correcto. Es por eso que decidir las recompensas es una parte crucial del Aprendizaje por Refuerzo.\n",
    "\n",
    "Recuperemos el código que ya desarrollamos en el método 1 (sin aprendizaje por refuerzo) para obtener los valores anteriores para este escenario y hacer la comparativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados después de 100 episodios:\n",
      "Número medio de acciones por episodio: 2564.85\n",
      "Número medio de penalizaciones por episodio: 830.44\n",
      "Recompensa media por episodio: -10017.81\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el comportamiento del agente sin Q-learning\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    # Crear el estado inicial\n",
    "    # state = env.encode(3, 1, 2, 0)\n",
    "    # env.s = state\n",
    "    env.s = (3, 1, 2, 0)\n",
    "    \n",
    "    # Inicializar las épocas, penalizaciones y recompensas\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "    actions = []\n",
    "    while not done:\n",
    "        # Elegir la acción random\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        # Ejecutar la accion\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        total_rewards += reward\n",
    "        # Actualiza el valor de penalizaciones si el reward es -10\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f'Resultados después de {episodes} episodios:')\n",
    "print(f'Número medio de acciones por episodio: {total_epochs / episodes}')\n",
    "print(f'Número medio de penalizaciones por episodio: {total_penalties / episodes}')\n",
    "print(f'Recompensa media por episodio: {total_rewards / episodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Medida** | **Rendimiento Agente Aleatorio** | **Rendimiento Agente Q-Learning** |\n",
    "|---------|---------|---------|\n",
    "| Número medio de acciones por episodio | 2564.85 | 13.22 |\n",
    "| Número medio de penalizaciones por episodio | 830.44 | 0.0 |\n",
    "| Recompensa media por episodio | -10017.81 | 7.78 |\n",
    "\n",
    "Como se puede obsrvar, los resultados del Q-Learning son claramente muy superiores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
